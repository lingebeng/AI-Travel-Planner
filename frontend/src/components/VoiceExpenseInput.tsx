/**
 * Voice Expense Input Component
 * Records voice and parses expense using AI
 */

import React, { useState, useRef } from 'react';
import { Button, message } from 'antd';
import { AudioOutlined, LoadingOutlined } from '@ant-design/icons';
import { expenseService, VoiceParseResult } from '../services/expenseService';
import { supabase } from '../lib/supabase';

interface VoiceExpenseInputProps {
  onParsed: (result: VoiceParseResult) => void;
  disabled?: boolean;
}

const VoiceExpenseInput: React.FC<VoiceExpenseInputProps> = ({
  onParsed,
  disabled = false,
}) => {
  const [isRecording, setIsRecording] = useState(false);
  const [isProcessing, setIsProcessing] = useState(false);
  const mediaRecorderRef = useRef<MediaRecorder | null>(null);
  const chunksRef = useRef<Blob[]>([]);

  const startRecording = async () => {
    try {
      const stream = await navigator.mediaDevices.getUserMedia({
        audio: {
          echoCancellation: true,
          noiseSuppression: true,
          autoGainControl: true,
        },
      });

      // æ£€æŸ¥æ˜¯å¦æ”¯æŒwebmæ ¼å¼
      const mimeType = MediaRecorder.isTypeSupported('audio/webm;codecs=opus')
        ? 'audio/webm;codecs=opus'
        : 'audio/webm';

      const mediaRecorder = new MediaRecorder(stream, {
        mimeType: mimeType,
      });

      mediaRecorderRef.current = mediaRecorder;
      chunksRef.current = [];

      mediaRecorder.ondataavailable = (event) => {
        if (event.data.size > 0) {
          chunksRef.current.push(event.data);
        }
      };

      mediaRecorder.onstop = async () => {
        const audioBlob = new Blob(chunksRef.current, { type: mimeType });

        // æ£€æŸ¥å½•éŸ³å¤§å°
        if (audioBlob.size < 1000) {
          message.warning('å½•éŸ³æ—¶é—´å¤ªçŸ­ï¼Œè¯·é‡æ–°å½•åˆ¶');
          stream.getTracks().forEach((track) => track.stop());
          return;
        }

        await processAudio(audioBlob);

        // Stop all tracks
        stream.getTracks().forEach((track) => track.stop());
      };

      mediaRecorder.start();
      setIsRecording(true);
      message.info('ğŸ¤ æ­£åœ¨å½•éŸ³... è¯·æ¸…æ™°è¯´å‡ºå¼€é”€å†…å®¹ï¼ˆå»ºè®®3-5ç§’ï¼‰', 5);
    } catch (error) {
      console.error('Failed to start recording:', error);
      message.error('æ— æ³•è®¿é—®éº¦å…‹é£ï¼Œè¯·æ£€æŸ¥æƒé™è®¾ç½®');
    }
  };

  const stopRecording = () => {
    if (mediaRecorderRef.current && isRecording) {
      mediaRecorderRef.current.stop();
      setIsRecording(false);
    }
  };

  const processAudio = async (audioBlob: Blob) => {
    setIsProcessing(true);

    try {
      // Get auth token from Supabase
      const { data: { session } } = await supabase.auth.getSession();
      if (!session) {
        throw new Error('æœªç™»å½•ï¼Œè¯·å…ˆç™»å½•');
      }

      // Step 1: Transcribe audio to text
      const formData = new FormData();
      formData.append('audio', audioBlob, 'recording.webm');

      const transcribeResponse = await fetch('http://localhost:5001/api/voice/transcribe', {
        method: 'POST',
        headers: {
          'Authorization': `Bearer ${session.access_token}`,
        },
        body: formData,
      });

      const transcribeData = await transcribeResponse.json();

      // åç«¯è¿”å›æ ¼å¼ï¼š{success: bool, error: string, transcription: string}
      if (!transcribeData.success || !transcribeData.transcription) {
        throw new Error(transcribeData.error || 'è¯­éŸ³è¯†åˆ«å¤±è´¥');
      }

      const voiceText = transcribeData.transcription;
      message.success(`è¯†åˆ«ç»“æœ: ${voiceText}`);

      // Step 2: Parse text into expense data using AI
      const parseResult = await expenseService.parseVoiceExpense(voiceText);

      // Check confidence
      if (parseResult.confidence < 0.5) {
        message.warning('AI è§£æç½®ä¿¡åº¦è¾ƒä½ï¼Œè¯·æ£€æŸ¥è¯†åˆ«ç»“æœ');
      }

      // Call parent callback
      onParsed(parseResult);
      message.success('å¼€é”€ä¿¡æ¯å·²è§£æ');
    } catch (error: any) {
      console.error('Failed to process audio:', error);
      message.error(error.message || 'å¤„ç†å¤±è´¥ï¼Œè¯·é‡è¯•');
    } finally {
      setIsProcessing(false);
    }
  };

  return (
    <Button
      type={isRecording ? 'primary' : 'default'}
      danger={isRecording}
      icon={isProcessing ? <LoadingOutlined /> : <AudioOutlined />}
      onClick={isRecording ? stopRecording : startRecording}
      disabled={disabled || isProcessing}
      size="large"
    >
      {isProcessing
        ? 'å¤„ç†ä¸­...'
        : isRecording
        ? 'åœæ­¢å½•éŸ³'
        : 'è¯­éŸ³è¾“å…¥'}
    </Button>
  );
};

export default VoiceExpenseInput;
